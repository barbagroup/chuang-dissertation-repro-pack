{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "import pathlib\n",
    "import numpy\n",
    "import scipy.interpolate\n",
    "import ipywidgets\n",
    "import pandas\n",
    "from h5py import File as h5open\n",
    "from cycler import cycler\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors\n",
    "from matplotlib import ticker\n",
    "from matplotlib.legend_handler import HandlerTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find helpers and locate workdir\n",
    "for parent in [pathlib.Path.cwd()] + list(pathlib.Path.cwd().parents):\n",
    "    if parent.joinpath(\"modulus\").is_dir():\n",
    "        projdir = parent\n",
    "        sys.path.insert(0, str(projdir.joinpath(\"modulus\")))\n",
    "        from helpers.utils import read_tensorboard_data  # pylint: disable=import-error\n",
    "        from helpers.lr_simulator import widget as lr_widget\n",
    "        from helpers.utils import log_parser  # pylint: disable=import-error\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Couldn't find module `helpers`.\")\n",
    "\n",
    "# point workdir to the correct folder\n",
    "workdir = projdir.joinpath(\"modulus\", \"tgv-2d-re100\")\n",
    "petibmdir = projdir.joinpath(\"petibm\", \"taylor-green-vortex-2d-re100\")\n",
    "figdir = workdir.joinpath(\"figures\")\n",
    "figdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified figure style\n",
    "pyplot.style.use(projdir.joinpath(\"resources\", \"figstyle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "nls = [1, 2, 3]\n",
    "nns = [16, 32, 64, 128, 256]\n",
    "nbss = [1024, 2048, 4096, 8192, 16384, 32768, 65536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_med_worst_base_cases():\n",
    "    \"\"\"Get the cases with the best, median, and the worst errors.\n",
    "    \"\"\"\n",
    "    bestl2norm = float(\"inf\")\n",
    "    best = None\n",
    "    errs = []\n",
    "    cases = []\n",
    "    for nl, nn, nbs in itertools.product(nls, nns, nbss):\n",
    "        with h5open(workdir.joinpath(\"outputs\", \"base-cases\", f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "            errs.append(float(h5file[f\"sterrs/u/l2norm\"][...]))\n",
    "        cases.append((nl, nn, nbs))\n",
    "    \n",
    "    errs = numpy.array(errs)\n",
    "    cases = numpy.array(cases, dtype=object)\n",
    "\n",
    "    besterr = numpy.min(errs)\n",
    "    bestconf = cases[numpy.argmin(errs)]\n",
    "    worsterr = numpy.max(errs)\n",
    "    worstconf = cases[numpy.argmax(errs)]\n",
    "    mederr = numpy.median(errs)\n",
    "    medconf = cases[numpy.where(errs == mederr)[0][0]]\n",
    "    \n",
    "    return (besterr, bestconf), (mederr, medconf), (worsterr, worstconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(besterr, bestconf), (mederr, medconf), (worsterr, worstconf) = get_best_med_worst_base_cases()\n",
    "print(f\"raw, (best case, err): {besterr}, {bestconf}\")\n",
    "print(f\"raw, (median case, err): {mederr}, {medconf}\")\n",
    "print(f\"raw, (worst case, err): {worsterr}, {worstconf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_base_case_training_history(workdir, h5dir, h5dir2, figdir, nl, nn, nbs, ws, close=False):\n",
    "    \"\"\"Plot figures related to training loss and spatial-temporal errors.\n",
    "    \"\"\"\n",
    "\n",
    "    data = log_parser(workdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}\"))\n",
    "\n",
    "    with h5open(h5dir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "        errsteps = h5file[\"walltime/steps\"][...]\n",
    "        errtimes = h5file[\"walltime/elapsedtimes\"][...]\n",
    "        err = [\n",
    "            h5file[\"walltime/l2norms/u/0.0\"][...],\n",
    "            h5file[\"walltime/l2norms/u/40.0\"][...],\n",
    "            h5file[\"walltime/l2norms/u/80.0\"][...],\n",
    "        ]\n",
    "\n",
    "    with h5open(h5dir2.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "        sterr = h5file[\"walltime/l2norms/u\"][...]\n",
    "\n",
    "    # plot according to optimizer type\n",
    "    fig, ax = pyplot.subplots(1, 1, sharex=False, sharey=False, figsize=(8, 4))\n",
    "    fig.suptitle(rf\"2D TGV, $Re=100$, $(N_l, N_n, N_{{bs}})=({nl}, {nn}, {nbs})$\")\n",
    "    \n",
    "    # loss against steps\n",
    "    ax.set_title(\"Training loss and solution errors v.s. iterations\")\n",
    "    l1, = ax.semilogy(data.index, data.loss.rolling(window=ws).min(), lw=2, label=\"Aggregated loss\")\n",
    "    l2, = ax.semilogy(errsteps, sterr, lw=2, ls=\"--\", label=r\"Overall spatial-temporal error\")\n",
    "\n",
    "    l4, = ax.semilogy(errsteps, err[0], lw=1.5, alpha=0.8, ls=\"--\", label=r\"$t=0$\")\n",
    "    l5, = ax.semilogy(errsteps, err[1], lw=1.5, alpha=0.8, ls=\"--\", label=r\"$t=40$\")\n",
    "    l6, = ax.semilogy(errsteps, err[2], lw=1.5, alpha=0.8, ls=\"--\", label=r\"$t=80$\")\n",
    "\n",
    "    # customized  legend locations\n",
    "    if (nl, nn, nbs) == (2, 32, 16384):\n",
    "        lloc1 = (0.55, 0.6)\n",
    "        lloc2 = (0.99, 0.6)\n",
    "    if (nl, nn, nbs) == (2, 32, 65536):\n",
    "        lloc1 = (0.55, 0.6)\n",
    "        lloc2 = (0.99, 0.6)\n",
    "    else:\n",
    "        lloc1 = (0.55, 0.99)\n",
    "        lloc2 = (0.99, 0.99)\n",
    "    \n",
    "    # legends\n",
    "    lgd1 = ax.legend(handles=[l1, l2], loc=\"upper right\", bbox_to_anchor=lloc1)\n",
    "    lgd2 = ax.legend(\n",
    "        handles=[l4, l5, l6], title=r\"Spatial error of $u$\",\n",
    "        loc=\"upper right\", bbox_to_anchor=lloc2, ncol=3\n",
    "    )\n",
    "    ax.add_artist(lgd1)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(r\"Loss or $L_2$ error\")\n",
    "    ax.grid()\n",
    "\n",
    "    # time axis / errors\n",
    "    axerr = ax.twiny()\n",
    "    axerr.semilogy(errtimes, err[0], lw=0)  # dummy line to set x-axis limits\n",
    "    axerr.spines[\"bottom\"].set_position((\"axes\", -0.3))\n",
    "    axerr.spines[\"bottom\"].set_visible(True)\n",
    "    axerr.xaxis.set_label_position(\"bottom\")\n",
    "    axerr.xaxis.set_ticks_position(\"bottom\")\n",
    "    axerr.set_xlabel(\"Run time (hours)\")\n",
    "    axerr.get_xaxis().set_ticks_position(\"bottom\")\n",
    "\n",
    "    # save\n",
    "    figdir.joinpath(\"training-hist\").mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(figdir.joinpath(\"training-hist\", f\"nl{nl}-nn{nn}-npts{nbs}.png\"))\n",
    "\n",
    "    if close:\n",
    "        pyplot.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nl, nn, nbs in itertools.product(nls, nns, nbss):\n",
    "#     print(nl, nn, nbs)\n",
    "#     plot_base_case_training_history(\n",
    "#         workdir=workdir.joinpath(\"base-cases\"),\n",
    "#         h5dir=workdir.joinpath(\"outputs\", \"base-cases\"),\n",
    "#         h5dir2=workdir.joinpath(\"old.nogit\", \"outputs.sterrs\", \"base-cases\"),\n",
    "#         figdir=figdir.joinpath(\"base-cases\"),\n",
    "#         nl=nl, nn=nn, nbs=nbs, ws=10, close=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot case training history\n",
    "option1 = ipywidgets.Dropdown(options=nls, value=3)\n",
    "option2 = ipywidgets.Dropdown(options=nns, value=256)\n",
    "option3 = ipywidgets.Dropdown(options=nbss, value=8192)\n",
    "option4 = ipywidgets.IntSlider(value=10, min=1, max=200, step=2, orientation=\"horizontal\") \n",
    "canvas = ipywidgets.interactive_output(\n",
    "    plot_base_case_training_history,\n",
    "    {\n",
    "        \"workdir\": ipywidgets.fixed(workdir.joinpath(\"base-cases\")),\n",
    "        \"h5dir\": ipywidgets.fixed(workdir.joinpath(\"outputs\", \"base-cases\")),\n",
    "        \"h5dir2\": ipywidgets.fixed(workdir.joinpath(\"old.nogit\", \"outputs.sterrs\", \"base-cases\")),\n",
    "        \"figdir\": ipywidgets.fixed(figdir.joinpath(\"base-cases\")),\n",
    "        \"nl\": option1, \"nn\": option2, \"nbs\": option3, \"ws\": option4,\n",
    "    }\n",
    ")\n",
    "\n",
    "out = ipywidgets.VBox([option1, option2, option3, option4, canvas])\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_base_case_contour(nl, nn, nbs, time, workdir, figdir, close=False):\n",
    "    \"\"\"Plot figures for the case performed the best at t=40.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5open(workdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "        coords = (h5file[\"field/x\"][...], h5file[\"field/y\"][...])\n",
    "\n",
    "        vals = {\n",
    "            r\"$u$\": h5file[f\"field/{time}/u\"][...],\n",
    "            r\"$v$\": h5file[f\"field/{time}/v\"][...],\n",
    "            r\"$p$\": h5file[f\"field/{time}/p\"][...],\n",
    "            r\"$\\omega_z$\": h5file[f\"field/{time}/vorticity_z\"][...],\n",
    "        }\n",
    "\n",
    "        errs = {\n",
    "            r\"$u$\": h5file[f\"field/{time}/err-u\"][...],\n",
    "            r\"$v$\": h5file[f\"field/{time}/err-v\"][...],\n",
    "            r\"$p$\": h5file[f\"field/{time}/err-p\"][...],\n",
    "            r\"$\\omega_z$\": h5file[f\"field/{time}/err-vorticity_z\"][...],\n",
    "        }\n",
    "    \n",
    "    # use analytical solutions for levels\n",
    "    lvls = {  # calculate solutions firstj\n",
    "        r\"$u$\": numpy.cos(coords[0]) * numpy.sin(coords[1]) * numpy.exp(-2.*0.01*float(time)),\n",
    "        r\"$v$\": - numpy.sin(coords[0]) * numpy.cos(coords[1]) * numpy.exp(-2.*0.01*float(time)),\n",
    "        r\"$p$\": - numpy.exp(-4.*0.01*float(time)) * (numpy.cos(2.*coords[0]) + numpy.cos(2.*coords[1])) / 4.,\n",
    "        r\"$\\omega_z$\": - 2. * numpy.cos(coords[0]) * numpy.cos(coords[1]) * numpy.exp(-2.*0.01*float(time)),\n",
    "    }\n",
    "\n",
    "    # actually convert them to levels\n",
    "    lvls = {field: numpy.linspace(val.min(), val.max(), 17) for field, val in lvls.items()}\n",
    "\n",
    "    # normalizer for error contourf\n",
    "    normerr = colors.LogNorm\n",
    "\n",
    "    # re-cal. the pressure w/ the mean from analytical soln. as it is assumed to have a constant shift\n",
    "    ptrue = - numpy.exp(-4.*0.01*float(time)) * (numpy.cos(2.*coords[0]) + numpy.cos(2.*coords[1])) / 4.\n",
    "    vals[r\"$p$\"] = vals[r\"$p$\"] - vals[r\"$p$\"].mean()\n",
    "    errs[r\"$p$\"] = abs(vals[r\"$p$\"] - ptrue)\n",
    "\n",
    "    fig, axs = pyplot.subplots(4, 2, sharex=True, sharey=True, figsize=(8.5, 13))\n",
    "\n",
    "    fig.suptitle(\n",
    "        rf\"Flow and errors, TGV 2D@$t={float(time)}$, $Re=100$, \"+\"\\n\" +\n",
    "        rf\"$(N_l, N_n, N_{{bs}})=({nl}, {nn}, {nbs})$\"\n",
    "    )\n",
    "\n",
    "    for i, ((field, err), val, lvl) in enumerate(zip(errs.items(), vals.values(), lvls.values())):\n",
    "        # field values\n",
    "        ct = axs[i, 0].contourf(*coords, val, lvl, extend=\"both\")\n",
    "        axs[i, 0].set_aspect(\"equal\")\n",
    "        axs[i, 0].set_title(field)\n",
    "        fig.colorbar(ct, ax=axs[i, 0], extend=\"both\")\n",
    "\n",
    "        # errors\n",
    "        ct = axs[i, 1].contourf(*coords, err, 17, extend=\"both\", norm=normerr(err.min(), err.max()))\n",
    "        axs[i, 1].set_aspect(\"equal\")\n",
    "        axs[i, 1].set_title(f\"Absolute error, {field}\")\n",
    "        fmt = ticker.LogFormatter()\n",
    "        cbar = fig.colorbar(ct, ax=axs[i, 1], format=fmt, extend=\"both\")\n",
    "    \n",
    "    axs[0, 0].set_ylabel(r\"$y$\")\n",
    "    axs[1, 0].set_ylabel(r\"$y$\")\n",
    "    axs[2, 0].set_ylabel(r\"$y$\")\n",
    "    axs[3, 0].set_ylabel(r\"$y$\")\n",
    "    axs[3, 0].set_xlabel(r\"$x$\")\n",
    "    axs[3, 1].set_xlabel(r\"$x$\")\n",
    "\n",
    "    figdir.joinpath(\"contours\").mkdir(parents=True, exist_ok=True)\n",
    "    pyplot.savefig(figdir.joinpath(\"contours\", f\"nl{nl}-nn{nn}-npts{nbs}-t{time}.png\"))\n",
    "\n",
    "    if close:\n",
    "        pyplot.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nl, nn, nbs in itertools.product(nls, nns, nbss):\n",
    "#     plot_base_case_contour(\n",
    "#         nl=nl, nn=nn, nbs=nbs, time=\"40.0\",\n",
    "#         workdir=workdir.joinpath(\"outputs\", \"base-cases\"),\n",
    "#         figdir=figdir.joinpath(\"base-cases\"), close=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot contours for cases\n",
    "option1 = ipywidgets.Dropdown(options=nls, value=2)\n",
    "option2 = ipywidgets.Dropdown(options=nns, value=32)\n",
    "option3 = ipywidgets.Dropdown(options=nbss, value=65536)\n",
    "option4 = ipywidgets.Dropdown(options=[\"0.0\", \"40.0\", \"80.0\"], value=\"40.0\")\n",
    "canvas = ipywidgets.interactive_output(\n",
    "    plot_base_case_contour,\n",
    "    {\n",
    "        \"nl\": option1, \"nn\": option2, \"nbs\": option3, \"time\": option4,\n",
    "        \"workdir\": ipywidgets.fixed(workdir.joinpath(\"outputs\", \"base-cases\")),\n",
    "        \"figdir\": ipywidgets.fixed(figdir.joinpath(\"base-cases\"))\n",
    "    }\n",
    ")\n",
    "display(ipywidgets.VBox([option1, option2, option3, option4, canvas]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_err_arch_boxplot(field, workdir, figdir):\n",
    "    \"\"\"plot_err_arch_boxplot\n",
    "    \"\"\"\n",
    "    data = {\"nl\": [], \"nn\": [], \"nbs\": [], \"l2norm\": []}\n",
    "    for nl, nn, nbs in itertools.product(nls, nns, nbss):\n",
    "        data[\"nl\"].append(nl)\n",
    "        data[\"nn\"].append(nn)\n",
    "        data[\"nbs\"].append(nbs)\n",
    "        with h5open(workdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "            data[\"l2norm\"].append(float(h5file[f\"sterrs/{field}/l2norm\"][...]))\n",
    "\n",
    "    data = pandas.DataFrame(data)\n",
    "    data = data.pivot(index=\"nbs\", columns=[\"nl\", \"nn\"], values=\"l2norm\")\n",
    "    data = data[data.mean().sort_values(ascending=False).index]\n",
    "\n",
    "    fig, ax = pyplot.subplots(1, 1)\n",
    "    fig.suptitle(r\"Error distribution across network architectures, \" rf\"${field}$\")\n",
    "\n",
    "    ax.boxplot(\n",
    "        data.values, labels=data.columns, showmeans=True,\n",
    "        meanprops={\"marker\": \".\", \"mfc\": \"k\", \"mec\": \"k\"},\n",
    "        medianprops={\"ls\": \"none\"},\n",
    "    )\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "    ax.set_xlabel(r\"$(N_l, N_n)$\")\n",
    "    ax.set_ylabel(rf\"$L_2$ error of ${field}$\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    figdir.joinpath(\"err-vs-arch\").mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(figdir.joinpath(\"err-vs-arch\", f\"err-arch-boxplot-{field}.png\"))\n",
    "\n",
    "plot_err_arch_boxplot(\"u\", workdir.joinpath(\"outputs\", \"base-cases\"), figdir.joinpath(\"base-cases\"))\n",
    "plot_err_arch_boxplot(\"v\", workdir.joinpath(\"outputs\", \"base-cases\"), figdir.joinpath(\"base-cases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dof_calculator(nl, nn, dim=2, unsteady=True, periodic=True):\n",
    "    \"\"\"Degree of freedom calculator.\n",
    "    \"\"\"\n",
    "    ninp = dim + int(unsteady) + 2 * int(periodic)\n",
    "    dof = ninp * nn + 2 * nn\n",
    "    dof += (nn**2 + 2 * nn) * (nl - 1)\n",
    "    dof += (nn + 1) * (dim + 1) \n",
    "    return dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dof_err(field, workdir, figdir):\n",
    "    \"\"\"plot_dof_err\n",
    "    \"\"\"\n",
    "    data = {\"dofs\": [], \"l2norm\": [], \"nbs\": []}\n",
    "    for nl, nn, nbs in itertools.product(nls, nns, nbss):\n",
    "        data[\"nbs\"].append(nbs)\n",
    "        data[\"dofs\"].append(dof_calculator(nl, nn, 2, True, True))\n",
    "        with h5open(workdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "            data[\"l2norm\"].append(float(h5file[f\"sterrs/{field}/l2norm\"][...]))\n",
    "\n",
    "    data = pandas.DataFrame(data)\n",
    "    data = data.pivot(index=\"nbs\", columns=[\"dofs\"], values=\"l2norm\")\n",
    "    data = data.sort_index(axis=1)\n",
    "\n",
    "    # box widths on the plot with log x axis\n",
    "    width = lambda p, w: 10**(numpy.log10(p)+w/2.)-10**(numpy.log10(p)-w/2.)\n",
    "\n",
    "    fig, ax = pyplot.subplots(1, 1)\n",
    "    fig.suptitle(r\"Error distribution v.s. degree of freedom, \" rf\"${field}$\")\n",
    "\n",
    "    ax.boxplot(\n",
    "        data.values, labels=data.columns, positions=data.columns,\n",
    "        showmeans=True, widths=width(data.columns, 0.1),\n",
    "        meanprops={\"marker\": \".\", \"mfc\": \"k\", \"mec\": \"k\"},\n",
    "        medianprops={\"ls\": \"none\"},\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Degree of freedom\")\n",
    "    ax.set_xscale(\"log\")\n",
    "\n",
    "    ax.set_ylabel(rf\"$L_2$ error of ${field}$\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    figdir.joinpath(\"err-vs-arch\").mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(figdir.joinpath(\"err-vs-arch\", f\"err-dof-boxplot-{field}.png\"))\n",
    "\n",
    "plot_dof_err(\"u\", workdir.joinpath(\"outputs\", \"base-cases\"), figdir.joinpath(\"base-cases\"))\n",
    "plot_dof_err(\"v\", workdir.joinpath(\"outputs\", \"base-cases\"), figdir.joinpath(\"base-cases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(lr_widget())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_loss_scatter(workdir, outdir, figdir, field):\n",
    "    \"\"\"Plot error vs loss.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = []\n",
    "    errs = []\n",
    "    ccodes = []\n",
    "    cmap = {(nl, nn): i for i, (nl, nn) in enumerate(itertools.product(nls, nns))}\n",
    "    invcmap = {i: (nl, nn) for i, (nl, nn) in enumerate(itertools.product(nls, nns))}\n",
    "\n",
    "    for (nl, nn, nbs) in itertools.product(nls, nns, nbss):\n",
    "        loss.append(log_parser(workdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}\")).iloc[-1][\"loss\"])\n",
    "\n",
    "        with h5open(outdir.joinpath(f\"nl{nl}-nn{nn}-npts{nbs}-raw.h5\"), \"r\") as h5file:\n",
    "            errs.append(float(h5file[f\"sterrs/{field}/l2norm\"][...]))\n",
    "        \n",
    "        ccodes.append(cmap[(nl, nn)])\n",
    "    \n",
    "    fig, ax = pyplot.subplots(1, 1)\n",
    "    fig.suptitle(rf\"Error v.s. aggregated loss, ${field}$\")\n",
    "\n",
    "    scatter = ax.scatter(loss, errs, c=ccodes, s=75, cmap=\"tab20\", alpha=0.6, marker=\"o\")\n",
    "\n",
    "    ax.set_xlabel(\"Aggregate loss\")\n",
    "    ax.set_xscale(\"log\")\n",
    "\n",
    "    ax.set_ylabel(rf\"$L_2$ error of ${field}$\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    # legend\n",
    "    handles, labels = scatter.legend_elements(prop=\"colors\", num=None, fmt=\"{x:d}\", func=lambda x: x.astype(int))\n",
    "    labels = [invcmap[int(key)] for key in labels]\n",
    "    ax.legend(handles, labels, loc=0, title=r\"$(N_l, N_n)$\", ncol=2)\n",
    "\n",
    "plot_error_loss_scatter(\n",
    "    workdir.joinpath(\"base-cases\"), workdir.joinpath(\"outputs\", \"base-cases\"),\n",
    "    workdir.joinpath(\"figures\"), \"u\"\n",
    ")\n",
    "\n",
    "plot_error_loss_scatter(\n",
    "    workdir.joinpath(\"base-cases\"), workdir.joinpath(\"outputs\", \"base-cases\"),\n",
    "    workdir.joinpath(\"figures\"), \"v\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalability for nl3-nn128-npts8192\n",
    "gpus = [1, 2, 4, 8]\n",
    "logdata = []\n",
    "h5data = []\n",
    "compdata = []\n",
    "comph5data = []\n",
    "\n",
    "for ngpus in gpus:\n",
    "\n",
    "    bname = \"exp-sum-scaling\"\n",
    "    cname = f\"nl3-nn128-npts8192-ngpus{ngpus}\"\n",
    "    logdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        h5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "    bname = \"base-cases\"\n",
    "    cname = f\"nl3-nn128-npts{8192*ngpus}\"\n",
    "    compdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        comph5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "styles_def = (\n",
    "    cycler(\"ls\", [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]*5) + \n",
    "    cycler(\"color\", pyplot.get_cmap(\"tab10\").colors*2) + \n",
    "    cycler(\"lw\", [2.0]*20) +\n",
    "    cycler(\"alpha\", [0.75]*20)\n",
    ")\n",
    "\n",
    "styles = styles_def()\n",
    "\n",
    "# ===============================================================\n",
    "fig, ax = pyplot.subplots(1, 1, figsize=(8, 4))\n",
    "fig.suptitle(\"Aggregated loss and elapsed time v.s. iteration\")\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "ax.grid()\n",
    "tax = ax.twinx()\n",
    "tax.set_ylabel(\"Run time (hours)\")\n",
    "\n",
    "l1s, l2s, l3s = [], [], []\n",
    "for ilogdata, ih5data in zip(logdata, h5data):\n",
    "    l1s.append(ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))[0])\n",
    "    l2s.append(ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))[0])\n",
    "    l3s.append(tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))[0])\n",
    "\n",
    "labels = [f\"{ngpus} GPUs\" for ngpus in gpus]\n",
    "lgds = [\n",
    "    ax.legend(l1s, labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(1.07, 1.025)),\n",
    "    ax.legend(l2s, labels, title=r\"$L_2$ err. of $u$\", loc=\"center left\", bbox_to_anchor=(1.07, 0.5)),\n",
    "    ax.legend(l3s, labels, title=\"Run time\", loc=\"lower left\", bbox_to_anchor=(1.07, -0.025))\n",
    "]\n",
    "ax.add_artist(lgds[0])\n",
    "ax.add_artist(lgds[1])\n",
    "ax.add_artist(lgds[2])\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl3-nn128-npts8192-scaling.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "# ===============================================================\n",
    "fig = pyplot.figure(figsize=(8, 6.5))\n",
    "fig.suptitle(r\"Comparing $(N_l, N_n, N_{bs}) \\times N_{gpu}$ and $(N_l, N_n, N_{bs} \\times N_{gpu})$\")\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "\n",
    "locs = [(0, slice(None, 2)), (0, slice(2, None)), (1, slice(None, 2))]\n",
    "for iloc, ngpus, ilogdata, icompdata, ih5data, icomph5data in zip(locs, gpus[1:], logdata[1:], compdata[1:], h5data[1:], comph5data[1:]):\n",
    "    styles = styles_def()\n",
    "    ax = fig.add_subplot(gs[iloc])\n",
    "    ax.set_title(rf\"$(3, 128, 8192) \\times {ngpus}$ v.s. $(3, 128, {8192*ngpus})$\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "    tax = ax.twinx()\n",
    "    tax.set_ylabel(\"Run time (hours)\")\n",
    "    l1, = ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))\n",
    "    l2, = ax.semilogy(icompdata.index, icompdata.loss.rolling(window=30).min(), **next(styles))\n",
    "    l3, = ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))\n",
    "    l4, = ax.semilogy(icomph5data[\"steps\"], icomph5data[\"l2norms\"], **next(styles))\n",
    "    l5, = tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))\n",
    "    l6, = tax.plot(icompdata.index, icompdata[\"time elapsed\"], **next(styles))\n",
    "\n",
    "# legend\n",
    "labels = [r\"$(N_l, N_n, N_{bs})\\times N_{gpu}$\", r\"$(N_l, N_n, N_{bs}\\times N_{gpu})$\"]\n",
    "fig.legend([l1, l2], labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.44), labelspacing=0.)\n",
    "fig.legend([l3, l4], labels, title=r\"$L_2$ err. of $u$\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.33), labelspacing=0.)\n",
    "fig.legend([l5, l5], labels, title=\"Run time\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.213), labelspacing=0.)\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl3-nn128-npts8192-multi-singl-gpus.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalability for nl2-nn32-npts8192\n",
    "gpus = [1, 2, 4, 8]\n",
    "logdata = []\n",
    "h5data = []\n",
    "compdata = []\n",
    "comph5data = []\n",
    "\n",
    "for ngpus in gpus:\n",
    "\n",
    "    bname = \"exp-sum-scaling\"\n",
    "    cname = f\"nl2-nn32-npts8192-ngpus{ngpus}\"\n",
    "    logdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        h5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "    bname = \"base-cases\"\n",
    "    cname = f\"nl2-nn32-npts{8192*ngpus}\"\n",
    "    compdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        comph5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "styles_def = (\n",
    "    cycler(\"ls\", [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]*5) + \n",
    "    cycler(\"color\", pyplot.get_cmap(\"tab10\").colors*2) + \n",
    "    cycler(\"lw\", [2.0]*20) +\n",
    "    cycler(\"alpha\", [0.75]*20)\n",
    ")\n",
    "\n",
    "styles = styles_def()\n",
    "\n",
    "# ===============================================================\n",
    "fig, ax = pyplot.subplots(1, 1, figsize=(8, 4))\n",
    "fig.suptitle(\"Aggregated loss and elapsed time v.s. iteration\")\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "ax.grid()\n",
    "tax = ax.twinx()\n",
    "tax.set_ylabel(\"Run time (hours)\")\n",
    "\n",
    "l1s, l2s, l3s = [], [], []\n",
    "for ilogdata, ih5data in zip(logdata, h5data):\n",
    "    l1s.append(ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))[0])\n",
    "    l2s.append(ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))[0])\n",
    "    l3s.append(tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))[0])\n",
    "\n",
    "labels = [f\"{ngpus} GPUs\" for ngpus in gpus]\n",
    "lgds = [\n",
    "    ax.legend(l1s, labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(1.07, 1.025)),\n",
    "    ax.legend(l2s, labels, title=r\"$L_2$ err. of $u$\", loc=\"center left\", bbox_to_anchor=(1.07, 0.5)),\n",
    "    ax.legend(l3s, labels, title=\"Run time\", loc=\"lower left\", bbox_to_anchor=(1.07, -0.025))\n",
    "]\n",
    "ax.add_artist(lgds[0])\n",
    "ax.add_artist(lgds[1])\n",
    "ax.add_artist(lgds[2])\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl2-nn32-npts8192-scaling.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "# ===============================================================\n",
    "fig = pyplot.figure(figsize=(8, 6.5))\n",
    "fig.suptitle(r\"Comparing $(N_l, N_n, N_{bs}) \\times N_{gpu}$ and $(N_l, N_n, N_{bs} \\times N_{gpu})$\")\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "\n",
    "locs = [(0, slice(None, 2)), (0, slice(2, None)), (1, slice(None, 2))]\n",
    "for iloc, ngpus, ilogdata, icompdata, ih5data, icomph5data in zip(locs, gpus[1:], logdata[1:], compdata[1:], h5data[1:], comph5data[1:]):\n",
    "    styles = styles_def()\n",
    "    ax = fig.add_subplot(gs[iloc])\n",
    "    ax.set_title(rf\"$(2, 32, 8192) \\times {ngpus}$ v.s. $(2, 32, {8192*ngpus})$\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "    tax = ax.twinx()\n",
    "    tax.set_ylabel(\"Run time (hours)\")\n",
    "    l1, = ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))\n",
    "    l2, = ax.semilogy(icompdata.index, icompdata.loss.rolling(window=30).min(), **next(styles))\n",
    "    l3, = ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))\n",
    "    l4, = ax.semilogy(icomph5data[\"steps\"], icomph5data[\"l2norms\"], **next(styles))\n",
    "    l5, = tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))\n",
    "    l6, = tax.plot(icompdata.index, icompdata[\"time elapsed\"], **next(styles))\n",
    "\n",
    "# legend\n",
    "labels = [r\"$(N_l, N_n, N_{bs})\\times N_{gpu}$\", r\"$(N_l, N_n, N_{bs}\\times N_{gpu})$\"]\n",
    "fig.legend([l1, l2], labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.44), labelspacing=0.)\n",
    "fig.legend([l3, l4], labels, title=r\"$L_2$ err. of $u$\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.33), labelspacing=0.)\n",
    "fig.legend([l5, l5], labels, title=\"Run time\", loc=\"upper left\", bbox_to_anchor=(0.6, 0.213), labelspacing=0.)\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl2-nn32-npts8192-multi-singl-gpus.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong scaling for nl3-nn128-npts65536\n",
    "gpus = [1, 2, 4, 8]\n",
    "logdata = []\n",
    "h5data = []\n",
    "\n",
    "for ngpus in gpus:\n",
    "\n",
    "    bname = \"exp-sum-scaling\"\n",
    "    cname = f\"nl3-nn128-npts{65536//ngpus}-ngpus{ngpus}\"\n",
    "    logdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        h5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "styles_def = (\n",
    "    cycler(\"ls\", [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]*5) + \n",
    "    cycler(\"color\", pyplot.get_cmap(\"tab10\").colors*2) + \n",
    "    cycler(\"lw\", [2.0]*20) +\n",
    "    cycler(\"alpha\", [0.75]*20)\n",
    ")\n",
    "\n",
    "styles = styles_def()\n",
    "\n",
    "# ===============================================================\n",
    "fig, ax = pyplot.subplots(1, 1, figsize=(8, 4))\n",
    "fig.suptitle(\"Aggregated loss and elapsed time v.s. iteration\")\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "ax.grid()\n",
    "tax = ax.twinx()\n",
    "tax.set_ylabel(\"Run time (hours)\")\n",
    "\n",
    "l1s, l2s, l3s = [], [], []\n",
    "for ilogdata, ih5data in zip(logdata, h5data):\n",
    "    l1s.append(ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))[0])\n",
    "    l2s.append(ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))[0])\n",
    "    l3s.append(tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))[0])\n",
    "\n",
    "labels = [f\"{ngpus} GPUs\" for ngpus in gpus]\n",
    "lgds = [\n",
    "    ax.legend(l1s, labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(1.07, 1.025)),\n",
    "    ax.legend(l2s, labels, title=r\"$L_2$ err. of $u$\", loc=\"center left\", bbox_to_anchor=(1.07, 0.5)),\n",
    "    ax.legend(l3s, labels, title=\"Run time\", loc=\"lower left\", bbox_to_anchor=(1.07, -0.025))\n",
    "]\n",
    "ax.add_artist(lgds[0])\n",
    "ax.add_artist(lgds[1])\n",
    "ax.add_artist(lgds[2])\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl3-nn128-npts65536-strong-scaling.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong scaling for nl2-nn32-npts65536\n",
    "gpus = [1, 2, 4, 8]\n",
    "logdata = []\n",
    "h5data = []\n",
    "\n",
    "for ngpus in gpus:\n",
    "\n",
    "    bname = \"exp-sum-scaling\"\n",
    "    cname = f\"nl2-nn32-npts{65536//ngpus}-ngpus{ngpus}\"\n",
    "    logdata.append(log_parser(workdir.joinpath(bname, cname)))\n",
    "\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        h5data.append({\n",
    "            \"steps\": h5file[\"walltime/steps\"][...],\n",
    "            \"elapsedtimes\": h5file[\"walltime/elapsedtimes\"][...],\n",
    "            \"l2norms\": h5file[\"walltime/l2norms/u/40.0\"][...]\n",
    "        })\n",
    "\n",
    "styles_def = (\n",
    "    cycler(\"ls\", [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]*5) + \n",
    "    cycler(\"color\", pyplot.get_cmap(\"tab10\").colors*2) + \n",
    "    cycler(\"lw\", [2.0]*20) +\n",
    "    cycler(\"alpha\", [0.75]*20)\n",
    ")\n",
    "\n",
    "styles = styles_def()\n",
    "\n",
    "# ===============================================================\n",
    "fig, ax = pyplot.subplots(1, 1, figsize=(8, 4))\n",
    "fig.suptitle(\"Aggregated loss and elapsed time v.s. iteration\")\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(r\"Aggregated loss and $L_2$ error of $u$\")\n",
    "ax.grid()\n",
    "tax = ax.twinx()\n",
    "tax.set_ylabel(\"Run time (hours)\")\n",
    "\n",
    "l1s, l2s, l3s = [], [], []\n",
    "for ilogdata, ih5data in zip(logdata, h5data):\n",
    "    l1s.append(ax.semilogy(ilogdata.index, ilogdata.loss.rolling(window=30).min(), **next(styles))[0])\n",
    "    l2s.append(ax.semilogy(ih5data[\"steps\"], ih5data[\"l2norms\"], **next(styles))[0])\n",
    "    l3s.append(tax.plot(ilogdata.index, ilogdata[\"time elapsed\"], **next(styles))[0])\n",
    "\n",
    "labels = [f\"{ngpus} GPUs\" for ngpus in gpus]\n",
    "lgds = [\n",
    "    ax.legend(l1s, labels, title=\"Loss\", loc=\"upper left\", bbox_to_anchor=(1.07, 1.025)),\n",
    "    ax.legend(l2s, labels, title=r\"$L_2$ err. of $u$\", loc=\"center left\", bbox_to_anchor=(1.07, 0.5)),\n",
    "    ax.legend(l3s, labels, title=\"Run time\", loc=\"lower left\", bbox_to_anchor=(1.07, -0.025))\n",
    "]\n",
    "ax.add_artist(lgds[0])\n",
    "ax.add_artist(lgds[1])\n",
    "ax.add_artist(lgds[2])\n",
    "\n",
    "# save\n",
    "figdir.joinpath(\"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figdir.joinpath(\"scaling-tests\", \"nl2-nn32-npts65536-strong-scaling.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak scaling table\n",
    "gpus = [1, 2, 4, 8]\n",
    "cases = [(2, 32, 8192), (3, 128, 8192)]\n",
    "indices = [\"Time cost\", \"Efficiency\", \"Loss\", r\"$L_2$ err., $u$\", r\"$L_2$ err., $v$\"]\n",
    "columns = pandas.MultiIndex.from_product([cases, gpus])\n",
    "columns = columns.set_names(\"GPUs\", level=1)\n",
    "data = pandas.DataFrame(data=None, index=indices, columns=columns)\n",
    "\n",
    "bname = \"exp-sum-scaling\"\n",
    "for (nl, nn, nbs), ngpus in itertools.product(cases, gpus):\n",
    "    cname = f\"nl{nl}-nn{nn}-npts{nbs}-ngpus{ngpus}\"\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        data.loc[\"Time cost\", ((nl, nn, nbs), ngpus)] = h5file[\"walltime/elapsedtimes\"][-1]\n",
    "        data.loc[r\"$L_2$ err., $u$\", ((nl, nn, nbs), ngpus)] = float(h5file[\"sterrs/u/l2norm\"][...])\n",
    "        data.loc[r\"$L_2$ err., $v$\", ((nl, nn, nbs), ngpus)] = float(h5file[\"sterrs/v/l2norm\"][...])\n",
    "    data.loc[\"Loss\", ((nl, nn, nbs), ngpus)] = log_parser(workdir.joinpath(bname, cname)).loc[400000, \"loss\"]\n",
    "    data.loc[\"Efficiency\", ((nl, nn, nbs), ngpus)] = \\\n",
    "        100 * data.loc[\"Time cost\", ((nl, nn, nbs), 1)].values[0] / \\\n",
    "            data.loc[\"Time cost\", ((nl, nn, nbs), ngpus)].values[0]\n",
    "\n",
    "out = data.style\n",
    "out = out.format(formatter=\"{:5.2f}\", subset=pandas.IndexSlice[\"Time cost\", :])\n",
    "out = out.format(formatter=\"{:2.0f}\", subset=pandas.IndexSlice[\"Efficiency\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[\"Loss\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[r\"$L_2$ err., $u$\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[r\"$L_2$ err., $v$\", :])\n",
    "\n",
    "out = out.to_latex(\n",
    "    column_format=\"lcccccccc\",\n",
    "    position=\"H\",\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    "    label=\"table:weak-scaling-perf\",\n",
    "    caption=(\n",
    "        \"\\n    Weak scaling performance for $(N_l, N_n, N_{bs})$ $=$ $(2, 32, 65536)$ and $(3, 128, 65536)$.%\\n\"\n",
    "        \"    Time costs denote the wall time required to finish 400k training iterations in hours.%\\n\"\n",
    "        \"    Efficiency here stands for weak scaling efficiency in $\\%$.%\\n\"\n",
    "        \"    The aggregated losses were those at the last iteration.%\\n\"\n",
    "        \"    The $L_2$ errors were the overall spatial-temporal errors at the last training iteration.%\\n\",\n",
    "        \"\\n    Weak scaling performance for $(N_l, N_n, N_{bs})=(2, 32, 65536)$ and $(3, 128, 65536)$\\n\"\n",
    "    ),\n",
    "    multicol_align=\"c\",\n",
    ")\n",
    "\n",
    "patn = r\"\\\\multicolumn(?:\\{.*?\\}){3}\"\n",
    "patn = rf\"(^\\s*?&\\s*?{patn}\\s*&\\s*{patn}.*?$)\"\n",
    "repl = r\"\\g<1>\\n\\\\cmidrule(rl){2-5} \\\\cmidrule(rl){6-9}\"\n",
    "out = re.sub(patn, repl, out, flags=re.MULTILINE)\n",
    "\n",
    "out = re.sub(r\"^(\\\\centering)$\", r\"\\g<1>\\n\\\\singlespacing\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"GPUs\", \"\\\\\\\\multicolumn{1}{r}{GPUs}\", out)\n",
    "out = re.sub(r\"(^Time cost.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^Efficiency.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^Loss.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^\\$L_2\\$ err\\., \\$u\\$.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "\n",
    "print(out)\n",
    "\n",
    "workdir.joinpath(\"tables\", \"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "with open(workdir.joinpath(\"tables\", \"scaling-tests\", \"weak-scaling.tex\"), \"w\") as fobj:\n",
    "    fobj.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong scaling table\n",
    "gpus = [1, 2, 4, 8]\n",
    "cases = [(2, 32, 65536), (3, 128, 65536)]\n",
    "indices = [\"Time cost\", \"Efficiency\", \"Loss\", r\"$L_2$ err., $u$\", r\"$L_2$ err., $v$\"]\n",
    "columns = pandas.MultiIndex.from_product([cases, gpus])\n",
    "columns = columns.set_names(\"GPUs\", level=1)\n",
    "data = pandas.DataFrame(data=None, index=indices, columns=columns)\n",
    "\n",
    "bname = \"exp-sum-scaling\"\n",
    "for (nl, nn, nbs), ngpus in itertools.product(cases, gpus):\n",
    "    cname = f\"nl{nl}-nn{nn}-npts{nbs//ngpus}-ngpus{ngpus}\"\n",
    "    with h5open(workdir.joinpath(\"outputs\", bname, f\"{cname}-raw.h5\"), \"r\") as h5file:\n",
    "        data.loc[\"Time cost\", ((nl, nn, nbs), ngpus)] = h5file[\"walltime/elapsedtimes\"][-1]\n",
    "        data.loc[r\"$L_2$ err., $u$\", ((nl, nn, nbs), ngpus)] = float(h5file[\"sterrs/u/l2norm\"][...])\n",
    "        data.loc[r\"$L_2$ err., $v$\", ((nl, nn, nbs), ngpus)] = float(h5file[\"sterrs/v/l2norm\"][...])\n",
    "    data.loc[\"Loss\", ((nl, nn, nbs), ngpus)] = log_parser(workdir.joinpath(bname, cname)).loc[400000, \"loss\"]\n",
    "    data.loc[\"Efficiency\", ((nl, nn, nbs), ngpus)] = \\\n",
    "        100 * data.loc[\"Time cost\", ((nl, nn, nbs), 1)].values[0] / \\\n",
    "            data.loc[\"Time cost\", ((nl, nn, nbs), ngpus)].values[0]\n",
    "\n",
    "out = data.style\n",
    "out = out.format(formatter=\"{:5.2f}\", subset=pandas.IndexSlice[\"Time cost\", :])\n",
    "out = out.format(formatter=\"{:2.0f}\", subset=pandas.IndexSlice[\"Efficiency\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[\"Loss\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[r\"$L_2$ err., $u$\", :])\n",
    "out = out.format(formatter=\"{:.1e}\", subset=pandas.IndexSlice[r\"$L_2$ err., $v$\", :])\n",
    "\n",
    "out = out.to_latex(\n",
    "    column_format=\"lcccccccc\",\n",
    "    position=\"H\",\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    "    label=\"table:strong-scaling-perf\",\n",
    "    caption=(\n",
    "        \"\\n    Strong scaling performance for $(N_l, N_n, N_{bs})$ $=$ $(2, 32, 65536)$ and $(3, 128, 65536)$.%\\n\"\n",
    "        \"    Time costs denote the wall time required to finish 400k training iterations in hours.%\\n\"\n",
    "        \"    Efficiency here stands for strong scaling efficiency in $\\%$.%\\n\"\n",
    "        \"    The aggregated losses were those at the last iteration.%\\n\"\n",
    "        \"    The $L_2$ errors were the overall spatial-temporal errors at the last training iteration.%\\n\",\n",
    "        \"\\n    Strong scaling performance for $(N_l, N_n, N_{bs})=(2, 32, 65536)$ and $(3, 128, 65536)$\\n\"\n",
    "    ),\n",
    "    multicol_align=\"c\",\n",
    ")\n",
    "\n",
    "patn = r\"\\\\multicolumn(?:\\{.*?\\}){3}\"\n",
    "patn = rf\"(^\\s*?&\\s*?{patn}\\s*&\\s*{patn}.*?$)\"\n",
    "repl = r\"\\g<1>\\n\\\\cmidrule(rl){2-5} \\\\cmidrule(rl){6-9}\"\n",
    "out = re.sub(patn, repl, out, flags=re.MULTILINE)\n",
    "\n",
    "out = re.sub(r\"^(\\\\centering)$\", r\"\\g<1>\\n\\\\singlespacing\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"GPUs\", \"\\\\\\\\multicolumn{1}{r}{GPUs}\", out)\n",
    "out = re.sub(r\"(^Time cost.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^Efficiency.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^Loss.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "out = re.sub(r\"(^\\$L_2\\$ err\\., \\$u\\$.*?)$\", r\"\\g<1>\\n\\\\addlinespace\", out, flags=re.MULTILINE)\n",
    "\n",
    "print(out)\n",
    "\n",
    "workdir.joinpath(\"tables\", \"scaling-tests\").mkdir(parents=True, exist_ok=True)\n",
    "with open(workdir.joinpath(\"tables\", \"scaling-tests\", \"weak-scaling.tex\"), \"w\") as fobj:\n",
    "    fobj.write(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('modulus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d2f06bb247d746ec823ea9336c693995af707d84c221c7086d89f854987291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
