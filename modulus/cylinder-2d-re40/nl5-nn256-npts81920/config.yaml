defaults :
  - modulus_default
  - arch: [fully_connected,]
  - scheduler: CyclicLR
  - optimizer: adamswa-ncgswa
  - loss: sum
  - override training: adamswa-ncgswa
  - _self_

jit: True

save_filetypes: npz

debug: false

arch:
  fully_connected:
    layer_size: 256
    nr_layers: 5

scheduler:
  base_lr: 1e-6
  max_lr: 1e-3
  step_size_up: 2000
  step_size_down: 2000
  cycle_momentum: False

optimizer:
  adam:
    lr: 1.0e-3
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.0
    amsgrad: False

  ncg:
    max_iters: 10000
    gtol: 1.0e-4
    ftol: 1.0e-4
    error: False
    debug: True
    debug_print_freq: 100

  ncgswa:
    max_iters: 10000
    gtol: 1.0e-5
    ftol: 1.0e-5
    error: False
    debug: True
    debug_print_freq: 100

training:
  adam:
    max_steps : 100000
    rec_results_freq : 1000
    rec_inference_freq: 5000
    summary_freq: 100

  adamswa:
    max_steps : 0

  ncg:
    max_steps : 200
    rec_results_freq : 1
    save_network_freq: 1
    print_stats_freq: 1
    summary_freq: 1

  ncgswa:
    max_steps : 200
    rec_results_freq : 1
    save_network_freq: 1
    print_stats_freq: 1
    summary_freq: 1

batch_size:
  nbatches: 1000
  nbcx: 8192
  nbcy: 8192
  nic: 8192
  ncylinder: 8192
  npts: 81920

custom:
  x: [-5.0, 15.0]
  y: [-5.0, 5.0]
  t: [0., 20.]
  radius: 0.5
  nu: 0.025
  rho: 1.0
  uic: 1.0
  activation: tanh
