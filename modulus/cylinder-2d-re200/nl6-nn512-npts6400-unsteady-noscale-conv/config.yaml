defaults :
  - modulus_default
  - arch: [fully_connected,]
  - scheduler: CyclicLR
  - optimizer: adamswa-ncgswa
  - loss: sum
  - override training: adamswa-ncgswa
  - _self_

jit: True

save_filetypes: npz

debug: false

arch:
  fully_connected:
    layer_size: 512
    nr_layers: 6

scheduler:
  base_lr: 1.0e-6
  max_lr: 1.0e-2
  step_size_up: 5000
  step_size_down: 5000
  cycle_momentum: False
  mode: "exp_range"
  gamma: 0.9999915

optimizer:
  adam:
    lr: 1.0e-1
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.0
    amsgrad: False

  adamswa:
    lr: 1.0e-1
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.0
    amsgrad: False

training:
  adam:
    max_steps : 800000
    rec_results_freq : 1000
    rec_constraint_freq: 10000
    rec_inference_freq: 10000
    summary_freq: 100
    save_network_freq: 10000
    backup_checkpoint_freq: 10000

  adamswa:
    max_steps : 200000
    rec_results_freq : 1000
    rec_constraint_freq: 10000
    rec_inference_freq: 10000
    summary_freq: 100
    save_network_freq: 1000
    backup_checkpoint_freq: 10000

  ncg:
    max_steps : 0

  ncgswa:
    max_steps : 0

batch_size:
  nbatches: 1000
  nbcx: 640
  nbcy: 640
  nic: 6400
  ncylinder: 640
  npts: 6400

custom:
  x: [-8.0, 25.0]
  y: [-8.0, 8.0]
  t: [0., 200.]
  radius: 0.5
  nu: 0.005
  rho: 1.0
  uic: 1.0
  activation: silu
